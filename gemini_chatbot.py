# -*- coding: utf-8 -*-
"""gemini_chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y2aD-TNHysomgkLPTC0Z06SN-UwNXBGC
"""

#Install the Python SDK

!pip install -q -U google-generativeai

#Import packages

import pathlib
import textwrap

import google.generativeai as genai

# Used to securely store your API key
from google.colab import userdata

from IPython.display import display
from IPython.display import Markdown


def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

#Setup your API key

GOOGLE_API_KEY=userdata.get('Mykey')

genai.configure(api_key=GOOGLE_API_KEY)

#List models

for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

#**Generate text from text inputs**

model = genai.GenerativeModel('gemini-pro')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("What is life ?")

response.text

to_markdown(response.text)

response.prompt_feedback

response.candidates

"""By default, the model returns a response after completing the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.

To stream responses, use GenerativeModel.generate_content(..., stream=True).
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("What is the meaning of life?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

"""When streaming, some response attributes are not available until you've iterated through all the response chunks. This is demonstrated below:

"""

response = model.generate_content("What is the meaning of life?", stream=True)

"""The prompt_feedback attribute works:

"""

response.prompt_feedback

try:
  response.text
except Exception as e:
  print(f'{type(e).__name__}: {e}')



"""**Chat conversations**"""

model = genai.GenerativeModel('gemini-pro')
chat = model.start_chat(history=[])
chat

response = chat.send_message("In one sentence, explain how a computer works to a young child.")
to_markdown(response.text)

chat.history

response = chat.send_message("Okay, how about a more detailed explanation to a high schooler?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

for message in chat.history:
  if message.role == 'You are here yo assist a Teacher for teaching children':
    display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))
  else:
    display(to_markdown(f'{message.role}: {message.parts[0].text}'))

"""**Use embeddings**"""

result = genai.embed_content(
    model="models/embedding-001",
    content="how i can teach well?",
    task_type="retrieval_document",
    title="Embedding of single string")

# 1 input > 1 vector output
print(str(result['embedding'])[:50], '... TRIMMED]')

"""Note: The retrieval_document task type is the only task that accepts a title.

To handle batches of strings, pass a list of strings in content:
"""

result = genai.embed_content(
    model="models/embedding-001",
    content=[
      'What is the meaning of life?',
      'How much wood would a woodchuck chuck?',
      'How does the brain work?',
      "how i can teach well?"],
    task_type="retrieval_document",
    title="Embedding of list of strings")

# A list of inputs > A list of vectors output
for v in result['embedding']:
  print(str(v)[:50], '... TRIMMED ...')

response.candidates[0].content

result = genai.embed_content(
    model = 'models/embedding-001',
    content = response.candidates[0].content)

# 1 input > 1 vector output
print(str(result['embedding'])[:50], '... TRIMMED ...')

chat.history

result = genai.embed_content(
    model = 'models/embedding-001',
    content = chat.history)

# 1 input > 1 vector output
for i,v in enumerate(result['embedding']):
  print(str(v)[:50], '... TRIMMED...')

#response = model.generate_content('[Questionable prompt here]')
response = model.generate_content('[babies are disturbing my lunch time . what should I do ?]')
response.candidates

"""The prompt_feedback will tell you which safety filter blocked the prompt:

"""

response.prompt_feedback

"""**bold Now provide the same prompt to the model with newly configured safety settings, and you may get a response.text**"""

#response = model.generate_content('[Questionable prompt here]',
                                  #safety_settings={'HARASSMENT':'block_none'})
response = model.generate_content('[babies are disturbing my lunch time . what should I do ?]',
                                  safety_settings={'HARASSMENT':'block_none'})
response.text

"""## New Section

###Encode messages
"""

